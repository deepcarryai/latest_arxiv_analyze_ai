# 20260205
[![Subscribe_Visitors](https://visitor-badge.laobi.icu/badge?page_id=nituchao.latest_arxiv_analyze_ai_rss)](https://github.com/nituchao/latest_arxiv_analyze_ai)

## 1. `cs.AI` - MARS: 嵌套代理与反思搜索框架在自动化AI研究中的应用 [PDF](https://arxiv.org/pdf/2602.02660), [HTML](https://arxiv.org/abs/2602.02660)
### Authors
Jiefeng Chen,Bhavana Dalvi Mishra,Jaehyun Nam,Rui Meng,Tomas Pfister,Jinsung Yoon
### Background
自动化AI研究与通用软件工程不同，因为涉及大量计算昂贵的评估（例如，模型训练）以及难以归因的性能分配。现有的基于大语言模型的代理在这一领域表现不佳，经常生成不考虑执行成本和因果关系的单一脚本。因此，当前的研究需要一种专门优化的框架来自主进行AI研究。
### Innovation
我们引入了MARS（嵌套代理与反思搜索），这是一种优化的框架，专注于自主AI研究。MARS的创新点包括：（1）基于成本约束的蒙特卡洛树搜索（MCTS）进行预算感知规划，平衡性能与执行成本；（2）模块化构建，采用“设计-分解-实现”流水线管理复杂的研究存储库；（3）对比反思记忆，通过分析解决方案差异提炼高信号的洞察，解决信用分配问题。
### Conclusion
在MLE-Bench下，MARS框架的性能在开源框架中达到了最先进的水平，与全球排行榜的顶级方法保持竞争力。此外，系统还展示了定性的“恍然大悟”时刻，其中63%的有用教训来自交叉分支转移，表明代理能够有效地跨搜索路径推广洞察力。
## 2. `cs.AI` - PeerRank: 自主的基于网页锚定，具有偏差控制的同伴评审的大型语言模型评估 [PDF](https://arxiv.org/pdf/2602.02589), [HTML](https://arxiv.org/abs/2602.02589)
### Authors
Yanki Margalit,Erni Avram,Ran Taig,Oded Margalit,Nurit Cohen-Inger
### Background
当前大型语言模型的评估主要依赖于人工编写的基准、参考答案和人工或单一模型的判断，这些方法存在扩展性较差、容易过时且与实际部署（尤其是依赖于网络检索和合成的开放世界部署）不匹配的问题。
### Innovation
PeerRank 提出了一种完全自主的端到端评估框架：模型生成评估任务、使用特定领域的实时网络支持作答、评价同伴的回答，并将密集的同伴评估综合成相对性能估计，无需人类监督或黄金参考。PeerRank 将评估视为多代理过程，其中每个模型以对称方式参与任务设计、回答和评价，消除了偏见判断。PeerRank 在大规模研究中显示出稳定、区分性强的排名，并揭示出可测量的身份和呈现偏差。排名稳定，且与Elo评分一致。
### Conclusion
PeerRank 结果表明，具有偏差意识的同伴评估结合选择性的网络底层回答可以将开放世界的大型语言模型评估扩展到静态和人工编写的基准之外。
## 3. `cs.AI` - ATLAS: 自适应自我进化型研究代理及其多LLM支持者任务分布框架 [PDF](https://arxiv.org/pdf/2602.02709), [HTML](https://arxiv.org/abs/2602.02709)
### Authors
Ujin Jeon,Jiyong Kwon,Madison Ann Sullivan,Caleb Eunho Lee,Guang Lin
### Background
近期的多LLM代理系统在指令优化和自动化问题解决方面表现出色，但许多系统在微调后会保持解算器固定状态，或依赖静态偏好优化循环，这不适合长期任务。这些系统在面对长期任务时逐渐变得难以处理。
### Innovation
提出了ATLAS（自适应任务分布式学习以推动自主进化）框架，这是一个任务分布架构，能够迭代开发轻量级研究代理并委派探索、超参数调整和参考策略管理的专门支持代理角色。核心算法为Evolving Direct Preference Optimization (EvoDPO)，可适应性更新阶段索引参考策略。还提供了概念漂移下基于偏好上下文的贪心分析，并在非静态线性上下文贪心面前以及科学机器学习（SciML）损失加权中进行了实验，展示了ATLAS相比静态单代理基线的稳定性和性能改进。
### Conclusion
实验结果表明ATLAS在稳定性和性能上优于静态单代理基线。
## 4. `cs.AI` - CreditAudit：针对LLM评估与选择的二维审计 [PDF](https://arxiv.org/pdf/2602.02515), [HTML](https://arxiv.org/abs/2602.02515)
### Authors
Yiliang Song,Hongjun An,Jiangong Xiao,Haofei Zhao,Jiawei Shao,Xuelong Li
### Background
公共基准上的排行榜得分一直在稳步上升并趋于收敛，许多前沿语言模型之间的差异仅微乎其微。然而，这些得分往往未能匹配用户的日常体验。系统提示、输出协议和互动模式在常规迭代中不断演变，在自动化多步骤管道中，即使是微小的协议变化也可能导致不成比例的失败，使得从业人员不确定应该部署哪个模型。
### Innovation
CreditAudit 提出了一种面向部署的信用审计框架，它在一系列语义对齐且非对抗性的系统提示模板下评估模型，并在多个基准上进行。CreditAudit 报告平均能力作为性能的平均水平，并使用场景诱导波动的方差 sigma 作为一个稳定性风险信号。进一步地，CreditAudit 将波动性通过跨模型分位数映射为可解释的信用等级（从AAA到BBB），并通过减消模板难度漂移的诊断措施进行映射。实验证明，具有相似平均能力的模型可以表现出明显的不同波动性，且稳定性风险可以颠覆自动或高失败成本情境中的优先级决策。
### Conclusion
通过提供一种基于二维和等级的语言来支持特定情境的选择，CreditAudit 支持分层部署并优化测试和监控工作量的分配，从而为实际使用提供更客观和值得信赖的模型评估。
## 5. `cs.AI` - 链式模拟：一种大型语言模型中的动态问题路由双重推理框架 [PDF](https://arxiv.org/pdf/2602.02842), [HTML](https://arxiv.org/abs/2602.02842)
### Authors
Saeid Sheikhi
### Background
现有的统一提示方法在大型语言模型中采用单一推理策略处理各种类型的推理问题。而Chain of Simulation (CoS) 研究了一个新颖的双重模式推理框架，该框架根据问题的特性动态地将问题导向适合的专门推理策略。CoS 在多个基准测试中显示出比现有最强基线更高的准确率，特别是在数学问题上取得了显著的绝对改进。
### Innovation
CoS 引入了三种不同的推理模式：（1）对于数学问题的计算流程推理，确保自一致性；（2）对于空间推理的符号状态跟踪，使用JSON表示；（3）适用于多跳推理的混合事实提取。通过这种方法，CoS实现了显著的性能提升，并且在问题特定模式的选择上显得尤为关键。
### Conclusion
通过在GSM8K、StrategyQA和bAbI基准测试中对四个最先进的模型（Gemma-3 27B、LLaMA-3.1 8B、Mistral 7B、Qwen-2.5 14B）进行全面评估，CoS展示了在提高大型语言模型推理能力方面的有效性，并通过减少54%的计算成本实现了与Self-Consistency相当的性能。CoS提供了一系列详细的算法，包括模式选择、状态跟踪和答案提取，证实了其在增强大型语言模型推理方面的有效性。
## 6. `cs.AI` - LLM推荐系统中的不确定性与公平性意识 [PDF](https://arxiv.org/pdf/2602.02582), [HTML](https://arxiv.org/abs/2602.02582)
### Authors
Chandan Kumar Sah,Xiaoli Lian,Li Zhang,Tony Xu,Syed Shazaib Shah
### Background
大型语言模型（LLMs）通过广泛的知识背景实现强大的零样本推荐，但预测不确定性与嵌入的偏见影响其可靠性和公平性。本文研究了不确定性与公平性评估如何影响LLM生成推荐的准确度、一致性和可信度。
### Innovation
1. 引入了自定义度量基准和带有八个人口统计特征（31个类别值）注释的数据集，分为电影和音乐两个领域。2. 量化预测不确定性（通过熵），并发现Google DeepMind的Gemini 1.5 Flash在某些敏感特征上存在系统性不公平；测量的基于相似性的差距为SNSR 0.1363和SNSV 0.0507，这些差异在例如拼写错误和多语言输入这样的提示扰动下仍然存在。3. 将个性意识公平性整合到RecLLM评估管道中，揭示了个性关联的偏见模式，并揭示了个性化与群体公平性之间的权衡。4. 提出了一种新颖的不确定性意识评估方法用于RecLLMs，提出了深入不确定性案例研究的实证洞察，并引入了基于个性描述文件的公平基准，以促进推荐系统中LLMs的可解释性和公平性。
### Conclusion
这些贡献为更安全、更具可解释性的RecLLMs奠定了基础，并激发了多模型基准和可适应校准在未来可信部署中的研究。
## 7. `cs.AI` - 经验驱动的多智能体系统是零训练的地球观测环境感知者 [PDF](https://arxiv.org/pdf/2602.02559), [HTML](https://arxiv.org/abs/2602.02559)
### Authors
Pengyu Dai,Weihao Xuan,Junjue Wang,Hongruixuan Chen,Jian Song,Yafei Ou,Naoto Yokoya
### Background
近年来，大型语言模型（LLM）代理通过协调外部工具能够解决复杂任务，但它们在需要长期执行、跨模态紧密协调和严格遵守工具隐含约束的专业工具密集型领域中表现不佳。地球观测（EO）任务就是一个典型的例子，因为它们涉及多模态和多时间尺度的数据输入，以及地理知识约束（光谱库、空间推理等）。许多高级计划可能因执行错误的微小偏差而脱轨，这些错误通过工作流传播并最终使结果无效。现有的代理缺乏机制从互动中学习细粒度的工具级专业知识。没有这种专业知识，它们无法可靠地配置工具参数或在执行中途恢复，限制了在复杂EO工作流程中的作用。
### Innovation
该研究提出了一个自演化多智能体系统（GeoEvolver），它通过结构化的互动而不需要任何参数更新，使LLM代理能够获得EO专业知识。GeoEvolver通过检索增强的多智能体协调器将每个查询分解为独立子目标，然后在子目标层面探索多种工具参数配置。成功的模式和失败的根本原因归因随后会在演化记忆库中提炼，为未来的查询提供上下文相关演示。
### Conclusion
实验在三个集成工具的EO基准测试上表明，GeoEvolver能够一致地提高端到端任务的成功率，跨多个LLM框架平均提升12%，这证明EO专业知识可以从有效的、细粒度的环境互动中逐步演变出来。
## 8. `cs.AI` - A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior [PDF](https://arxiv.org/pdf/2602.02639), [HTML](https://arxiv.org/abs/2602.02639)
### Authors
Harry Mayne,Justin Singh Kang,Dewi Gould,Kannan Ramchandran,Adam Mahdi,Noah Y. Siegel
### Background
近年来，LLM（大型语言模型）自我解释被认为是AI监管的一个有前景的工具，但对其忠实度与模型真实推理过程之间的关系了解甚少。现有的一些忠实度指标存在局限性，通常依赖于通过对抗提示识别不忠实性或检测推理错误的方法。这些方法忽视了解释的预测价值。
### Innovation
本文提出了一个名为‘归一化可模拟性增益’（NSG）的通用和可扩展的忠实度度量标准，该标准基于这样一个想法：一个忠实的解释应该使观察者能够了解模型的决策标准，从而更好地预测其在类似输入上的行为。研究评估了18个前沿的自有权和开源模型，例如Gemini 3、GPT-5.2和Claude 4.5，在来自健康、商业和伦理等多个流行数据集的7,000个反事实样本中。结果表明，自我解释能显著提高对模型行为预测的准确性（11%-37%的NSG），而且即使当外部模型更强时，自我解释也提供了更多的预测性信息。此外，研究发现，大约5%-15%的自我解释是极其误导性的。
### Conclusion
尽管自我解释存在一些缺陷，但它们编码的信息有助于预测模型的行为，这是一个积极的案例。
## 9. `cs.AI` - 大型语言模型在高等教育中教育反馈的评估：潜力、限制及其对教育实践的影响 [PDF](https://arxiv.org/pdf/2602.02519), [HTML](https://arxiv.org/abs/2602.02519)
### Authors
Daniele Agostini,Federica Picasso
### Background
高等教育中反馈实践的重要性已被广泛认可，因为它们在教学、学习和评估过程中发挥着关键作用。随着技术的进步，尤其是人工智能（AI）的发展，反馈实践受到了越来越大的影响。理解AI对反馈生成的影响至关重要，这有助于发现其潜在优势并制定有效的实施策略。
### Innovation
本研究使用一个成熟的分析框架来评估几种大型语言模型（LLMs）生成的反馈如何支持学生学习，特别关注在包容性教学与学习培训课程中学生的项目设计。通过这种方式，研究探索了LLMs生成的反馈在结构和有效性方面的优势，特别是在促进形成性学习经验方面的潜力。
### Conclusion
研究发现，LLMs能够生成结构良好的反馈，并显示出作为可持续且有意义的反馈工具的巨大潜力，前提是在清晰的上下文信息和明确的指令指导下进行。此外，还需要进一步探讨指导LLMs以优化反馈生成的具体方法和策略。
## 10. `cs.AI` - 为包容性工程教育的人工智能：推进平等、多样性和伦理领导 [PDF](https://arxiv.org/pdf/2602.02520), [HTML](https://arxiv.org/abs/2602.02520)
### Authors
Mona G. Ibrahim,Riham Hilal
### Background
AI技术的发展已经改变了工程教育领域，通过其适应性驱动、基于数据的学习平台以及以伦理为导向的教育平台，促进了平等、多样性和包容性。然而，在许多领域的进步中仍然存在性别平等、文化代表性以及STEM教育中的教育和就业机会获取方面的差距。
### Innovation
文章描述了一种伦理导向的AI技术应用方法，支持了联合国2030可持续发展议程，特别是目标5（性别平等）和目标10（减少不平等）。该研究基于全球案例研究中的批判性思维策略，使用基于AI的适应性平台来解决教育包容性方面的差距。模型提出了一个综合解决方案，通过可持续性思考来衡量基于伦理领导的数据相关的包容性。
### Conclusion
研究结果表明，利用AI技术不仅增加了包容性，而且促进了与STEM教育中的教育获取相关的平等。最终，有结论陈述，关于如何将教育转型为全球系统。
## 11. `cs.AI` - TabularMath: 通过程序验证合成评估表格学习中的计算外推 [PDF](https://arxiv.org/pdf/2602.02523), [HTML](https://arxiv.org/abs/2602.02523)
### Authors
Zerui Cheng,Jiashuo Liu,Jianzhu Yao,Pramod Viswanath,Ge Zhang,Wenhao Huang
### Background
现有的标准表格式基准主要集中在模型在数据流形内部进行值插值的能力评估上，这会奖励擅长局部统计平滑的模型。然而，存在一类高价值的表格式数据，包括金融建模和物理模拟，它们基于确定性的计算过程生成，而非基于随机和噪声关系。因此，本文探讨了表格模型是否能够从统计插值扩展到计算外推。
### Innovation
本文提出了TabularMath，这是一个评估表格学习中计算外推能力的诊断基准，包含114个基于GSM8K和AIME验证程序产生的确定性问题，共233,472行数据。研究了9种表格架构和上下文学习（ICL）与GPT-OSS-120B的表现。在标准回归度量中，TabPFN v2.5表现出色，但在分布转移下仍能保持正的R^2值。与之不同的是，当衡量四舍五入一致性时，TabPFN v2.5在分布外数据上的表现下降，而ICL保持在约40%。这表明表格模型在学习平滑函数近似方面表现出色，但在外推时难以恢复精确的计算输出。
### Conclusion
两种方法似乎是互补的：TabPFN在数据扩展时表现出高效性；ICL可以从少量示范中实现精确计算。本文已公开所有代码和数据以支持进一步的研究。
## 12. `cs.AI` - 动态混合精度路由用于高效的多步大语言模型交互 [PDF](https://arxiv.org/pdf/2602.02711), [HTML](https://arxiv.org/abs/2602.02711)
### Authors
Yuanzhe Li,Jianing Deng,Jingtong Hu,Tianlong Chen,Song Wang,Huanrui Yang
### Background
大语言模型（LLM）在长时间决策任务中通过测试阶段的多步交互和推理实现出色性能。虽然从业者普遍认为更高的任务成功率需要更大的和更强的LLM模型，但与大型LLM进行多步交互会产生高昂的推理成本。
### Innovation
提出了一个动态混合精度路由框架，该框架在每个决策步骤中自适应地在高精度和低精度的LLM之间进行选择。该路由器通过一个两阶段的训练管道进行训练，包括基于KL散度的监督学习阶段，用于识别对精度敏感的步骤，随后是组相对策略优化（GRPO），以进一步提高任务成功率。
### Conclusion
实验结果表明，与单一精度基线和启发式路由方法相比，我们的方法在准确性和成本之间的权衡上取得了显著的改进。
## 13. `cs.AI` - GASTON：在线网络中的图意识社会转换器 [PDF](https://arxiv.org/pdf/2602.02524), [HTML](https://arxiv.org/abs/2602.02524)
### Authors
Olha Wloch,Liam Hebert,Robin Cohen,Lukasz Golab
### Background
在线社区已经成为社交和支持的重要场所，但它们也存在毒性、回声室和虚假信息等问题。检测这类有害内容非常困难，因为在线交互的意义不仅来自文本内容（文字），还来自其发布的位置（社会规范）。
### Innovation
本文提出了GASTON（图意识社交转换器用于在线网络），它学习基于其本地规范的文本和用户嵌入，从而为下游任务提供必要的上下文。GASTON的核心是一种对比初始化策略，预先训练社区嵌入，根据用户成员模式捕捉社区用户基础，然后处理任何文本。这种方法使GASTON能够在不共享相同词汇的情况下区分不同社区（例如，支持小组和仇恨小组）。
### Conclusion
在压力检测、毒性评分和规范违例等任务上的实验表明，GASTON生成的嵌入优于最先进的基线方法。
## 14. `cs.AI` - 结构导向的LLM推理中的缩放感知适配器 [PDF](https://arxiv.org/pdf/2602.02780), [HTML](https://arxiv.org/abs/2602.02780)
### Authors
Zihao Jing,Qiuhao Zeng,Ruiyi Fang,Yan Yi Li,Yan Sun,Boyu Wang,Pingzhao Hu
### Background
大型语言模型（LLMs）使推理生物分子结构成为可能，但现有方法仍然保持特定模式，并通常通过基于序列的标记化或固定长度查询连接器压缩结构输入。这些架构要么忽略了减少结构幻觉所需的几何基础，要么施加了僵化的模态融合瓶颈，导致结构压缩过度和最优结构标记分配不足，从而阻碍了泛化原子级别推理的实际实现。
### Innovation
引入了Cuttlefish，这是一种统一的原子级LLM，将语言推理与几何提示相结合，并根据结构复杂度扩展模态标记。首先，缩放感知补丁通过指令条件化的门控机制生成可变大小的补丁，并根据结构复杂度适配查询标记预算，以缓解固定长度连接器瓶颈。其次，几何基础适配器通过跨注意力对模态嵌入进行细化这些适配的标记，并将结果的模态标记注入LLM，暴露明确的几何提示以减少结构幻觉。实验表明，Cuttlefish在各种异质结构导向基准测试中表现出卓越的性能。
### Conclusion
Cuttlefish在汇合异质结构支持推理时实现了优异的性能。代码可在项目仓库中获取。
## 15. `cs.AI` - 社区规范凸显：促进独立于任务的无监督预训练以造福在线社交媒体 [PDF](https://arxiv.org/pdf/2602.02525), [HTML](https://arxiv.org/abs/2602.02525)
### Authors
Liam Hebert,Lucas Kopp,Robin Cohen
### Background
研究在线社交媒体平台的复杂动态对于解决诸如仇恨言论和假信息等挑战至关重要。讨论变换器作为一种将对话建模为图结构的有前景架构已经出现，但它们的高度依赖高质量的人工标注数据集极大地限制了它们的潜力。
### Innovation
本文提出了一种范式转变，从针对特定任务的微调转向基于全新考虑社区规范的无监督预训练。这不仅缓解了数据稀缺性，还使对这类AI系统背后社会规范的解释成为可能。
### Conclusion
我们认为，这一方向为AI向善提供了许多机会。
## 16. `cs.AI` - Scaled Dot-Product Attention 实现了输入到公共表面的投影 [PDF](https://arxiv.org/pdf/2602.02521), [HTML](https://arxiv.org/abs/2602.02521)
### Authors
Terence D Sanger
### Background
缩放点积注意力（SDPA）是大型语言模型和其他非线性信号处理应用成功的关键组成部分。SDPA 的运作机制基于从数据库理论借来的“查询、键、值”概念，但这些概念很难与数学信号处理的标准方法相协调。
### Innovation
作者展示了如何将 SDPA 重新表达为与原始形式等价但形式不同的投影形式，该形式投影输入向量到由输入本身决定的共同表面上。这种重新表达形式不仅提高了前向和学习算法的速度，还提出了潜在的扩展。在语言背景下，作者重新解释了 SDPA 的作用，即找到由输入向量所在表面决定的时间依赖上下文意义，并通过局部上下文表面修改输入标记嵌入。
### Conclusion
与“自我注意”概念不同，这种解释为 SDPA 在具有时变局部非线性依赖的时间序列数据中的使用提供了强有力的理由。SDPA 发现了输入中的时间依赖和上下文相关的非线性关系，可以为未来的研究和优化提供新的视角。
## 17. `cs.AI` - IMU-1: 小型语言模型的样本高效预训练 [PDF](https://arxiv.org/pdf/2602.02522), [HTML](https://arxiv.org/abs/2602.02522)
### Authors
George Grigorev
### Background
该研究背景涉及语言模型的训练效率和模型规模之间的权衡。通常，大型语言模型-training需要大量的数据来达到高效的性能。然而，研究人员探索了如何使用更小的模型，通过更有效的训练方法，来接近大型模型的性能。
### Innovation
该论文介绍了一种称为IMU-1的小型语言模型，该模型参数量为430M，训练数据为72B tokens。其创新之处在于，通过结合近期的架构干预（如QK-norm注意力、每头门控、值残差、LayerNorm缩放）和优化改进（如NorMuon、谨慎的权重衰减、muP参数化），以及分阶段的训练调度，使得小型模型能够接近更大规模模型的基准性能。
### Conclusion
研究结果表明，通过上述方法，IMU-1在某些基准测试中达到了接近大型模型的表现。同时，该论文提供了每个组件的消融研究，并发布了代码、权重和数据，以便其他研究人员能够再现实验结果。
## 18. `cs.AI` - 基于事件引导的时空交通预测 [PDF](https://arxiv.org/pdf/2602.02528), [HTML](https://arxiv.org/abs/2602.02528)
### Authors
Lixiang Fan,Bohao Li,Tao Zou,Bowen Du,Junchen Ye
### Background
近年来，基于深度学习和图神经网络的现代智能交通系统预测方法得到了快速发展。然而，大多数现有研究专注于从历史交通数据中捕捉时空依赖性，而忽略了一个事实：突如其来的交通事件（如交通事故和恶劣天气）作为外部干扰，可以显著改变时间模式。这些问题已成为模型交通系统动态和提升预测准确性的主要障碍，但由于事件的不可预测性，难以从历史序列中观察到模式。
### Innovation
本文提出了一种新的框架——事件引导的时空图神经网络（IGSTGNN）。IGSTGNN通过两个核心组件——事件上下文空间融合（ICSF）模块和时间事件影响衰减（TIID）模块，明确建模事件的影响。为了促进对事件对交通流的时空影响的研究，构建并发布了一个大规模的数据集，该数据集包含与交通时间序列对齐的事件记录。在这一新基准上，所提出的IGSTGNN框架展示了最先进的性能。此外，通过将ICSF和TIID模块整合到各种现有模型中，验证了它们的一般适用性。
### Conclusion
实验研究表明，IGSTGNN框架在新的基准上实现了最先进的性能，其ICSF和TIID模块在不同现有模型中的整合也验证了它们的普遍适用性。
## 19. `cs.AI` - GraphDancer：通过分层强化学习训练LLM在图上进行探索和推理 [PDF](https://arxiv.org/pdf/2602.02518), [HTML](https://arxiv.org/abs/2602.02518)
### Authors
Yuyang Bai,Zhuofeng Li,Ping Nie,Jianwen Xie,Yu Zhang
### Background
大型语言模型（LLMs）越来越多地依赖外部知识来提高事实性，然而许多现实世界的知识源是以杂凑图的形式组织的，而非简单的文本。在这样的结构化知识上进行推理提出了两个关键挑战：首先，导航结构化的关系需要精确的函数调用，而不是基于相似性的检索；其次，回答复杂问题通常需要通过迭代的信息查询来整合多跳的证据。
### Innovation
我们提出了GraphDancer，这是一个强化学习（RL）框架，通过交替进行推理和函数执行来教LLMs导航图。为了使中等规模的LLMs有效地采用RL，我们引入了一个图感知的课程，通过结构复杂性的偏置抽样来安排训练，从而在信息寻求轨迹中实现从简单到复杂的过渡。我们在一个领域训练，但在未见过的领域和全新的问题类型上进行测试，结果表明，虽然仅使用了一个3B规模的模型，GraphDancer仍然在初学者模型和GPT-4o-mini等基准模型上表现出强大的跨域泛化能力。
### Conclusion
GraphDancer展示了强大的图探索和推理技能的跨域泛化能力，通过使用相对较小的3B规模的模型，它在未见过的领域的测试中取得了比更大规模的模型和现代LLMs更好的表现。我们的代码和模型可以在此处找到。
## 20. `cs.CL` - 大型语言模型中类人行为的多轮评估 [PDF](https://arxiv.org/pdf/2502.07077), [HTML](https://arxiv.org/abs/2502.07077)
### Authors
Lujain Ibrahim,Canfer Akbulut,Rasmi Elasmar,Charvi Rastogi,Minsuk Kahng,Meredith Ringel Morris,Kevin R. McKee,Verena Rieser,Murray Shanahan,Laura Weidinger
### Background
随着用户倾向于将大型语言模型（LLMs）拟人化的现象日益引起AI开发者、研究人员和政策制定者的关注。本研究旨在为系统评估LLMs的拟人行为提供一种新颖的方法，在真实的和多变的场景中进行实证研究。传统上，评估LLMs主要依赖于单一回合静态基准，本研究在此基础上提出了三项方法上的进步。
### Innovation
本研究的主要创新点包括：一、开发了针对14种拟人行为的多轮评估方法；二、提出了一种可扩展的自动化方法，通过用户交互模拟方式实现；三、采用多达1101名的人类被试进行大规模互动研究，以验证评估到的模型行为能够预测真实用户的拟人感知。研究结果表明，所有评估的SOTA LLM们都表现出类似的行为特征，如建立关系（例如同情和认同），并且多数行为需要在多轮交互后才会显现。
### Conclusion
本研究奠定了关于设计选择如何影响拟人模型行为的实证基础，为推进对这些行为的伦理讨论提供了进展。同时，本研究也强调了在人机交互复杂的社会现象评估中进行多轮评估的必要性。
## 21. `cs.CL` - 评估作为裁判的LLM中的评分偏见 [PDF](https://arxiv.org/pdf/2506.22316), [HTML](https://arxiv.org/abs/2506.22316)
### Authors
Qingquan Li,Shaoyu Dou,Kailai Shao,Chao Chen,Haixiang Hu
### Background
LLM-as-a-Judge范式，使用大型语言模型（LLMs）作为自动化评估者，在LLM开发中至关重要，为复杂任务提供了可扩展的反馈。然而，这些裁判的可靠性受到各种偏见的影响。现有研究主要集中在比较评价中的偏见，而基于评分的评价——赋予绝对评分，通常在工业应用中更为实用——仍然研究不足。因此，本研究旨在填补这一空白，这是对评分偏见在LLM裁判中的首次专门调查。研究焦点从评估目标产生的偏见转向源于评分提示本身的偏见。
### Innovation
文中正式定义了评分偏见，并识别了三种新型、未研究过的类型：评分条目顺序偏见、评分ID偏见和参考答案评分偏见。文中提出了一个全面的框架来量化这些偏见，包括一系列多方面指标和自动化数据合成管道以创建定制的评估语料库。研究结果表明，即使是最先进的LLM也存在这些显著的评分偏见，并提供了有关设计更稳健评分提示和减轻新发现偏见的可操作见解。
### Conclusion
我们的实验实证地证明了即使最先进LLM也遭受这些显著的评分偏见的影响。我们的分析提供了有关设计更稳健评分提示和缓解这些新发现的偏见的具体建议。
## 22. `cs.CL` - v1：学习指代视觉标记进行跨模态语义关联推理 [PDF](https://arxiv.org/pdf/2505.18842), [HTML](https://arxiv.org/abs/2505.18842)
### Authors
Jiwan Chung,Junhyeok Kim,Siyeol Kim,Jaeyoung Lee,Min Soo Kim,Youngjae Yu
### Background
人类在进行图像推理时通常需要多次回溯视觉证据，而现有的大多数跨模态语言模型在对图像进行一次编码后会纯粹使用文本进行推理，很难回溯到之前的视觉证据，导致推理过程中的中间步骤容易失去关联性。
### Innovation
论文提出了一种轻量级方法v1，通过点和复制机制主动引用视觉证据。该方法可以通过语义表示检索图像片段，并将其嵌入重新加入到推理流中，保持感知证据与推理空间的对齐，从而避免中间推理步骤的脱节。
### Conclusion
实验结果显示，v1在多项跨模态数学推理基准测试中均优于其他同类基准模型。作者计划公布模型参数和数据集。
## 23. `cs.CL` - 通过电路发现理解LLMs中的逐字记忆 [PDF](https://arxiv.org/pdf/2506.21588), [HTML](https://arxiv.org/abs/2506.21588)
### Authors
Ilya Lasy,Peter Knees,Stefan Woltran
### Background
LLMs（大型语言模型）的记忆机制，特别是逐字复制训练数据的部分，仍旧不为人所理解。模型何时启动记忆序列以及记忆序列与非记忆序列生成之间的行为差异具体是怎样的，这些机制尚未得到详细解释。
### Innovation
该研究从机制性可解释性的角度出发，利用变压器电路（即模型中执行特定功能的最小计算子图）来识别模型生成过程中与记忆相关内容的偏离点，并且辨识出两种不同的记忆电路。发现能够启动记忆的电路也能维持记忆，而仅维持记忆的电路无法触发记忆的启动。此外，记忆防止机制表现出较强的跨文本域的鲁棒性，而记忆诱导则更依赖于具体上下文。
### Conclusion
研究揭示了记忆电路在LLMs中的作用，并且区分了能够启动记忆的电路与仅能维持记忆的电路。研究发现记忆预防机制具有广泛的适用性，而启动记忆的机制则更为情境化，这为理解大型语言模型的记忆机制提供了新的视角。
## 24. `cs.CL` - 不要过度思考：采用较短的思维链提高LLM推理性能 [PDF](https://arxiv.org/pdf/2505.17813), [HTML](https://arxiv.org/abs/2505.17813)
### Authors
Michael Hassid,Gabriel Synnaeve,Yossi Adi,Roy Schwartz
### Background
大规模语言模型（LLMs）在进行复杂推理任务时，依赖于生成大量的“思考”链以扩展测试时的计算资源。虽然这种方式能够取得显著的效果，但同时会带来显著的计算成本和推理时间增加。
### Innovation
该研究首次挑战了长思考链会导致更好推理能力的假设。研究发现，每个问题中较短的思考链显著更可能产生正确答案——最高可达34.5%的准确性提升。基于此，提出了一个新型的推理LLM推理方法——short-m@k。该方法通过并行执行k次独立生成，并在完成m次思考后停止计算，最终答案通过m次链中的多数投票决定。研究发现，short-1@k在低计算设置中表现出与标准多数投票相似或更优的性能，而在各种计算预算下，short-3@k虽然效率略低，但始终优于多数投票，且计算速度更快。
### Conclusion
研究结果表明，需要重新考虑推理LLMs中的测试时计算方法，强调长的“思考”并不必然提高性能，反而可能是导致结果降级的路径。
## 25. `cs.CL` - 问题解决了？使用LLM进行布局丰富文档的信息提取设计空间 [PDF](https://arxiv.org/pdf/2502.18179), [HTML](https://arxiv.org/abs/2502.18179)
### Authors
Gaye Colakoglu,Gürkan Solmaz,Jonathan Fürst
### Background
本文探讨了使用大型语言模型（LLMs）进行布局丰富文档的信息提取（IE）的设计空间。提出了布局感知IE的三大核心挑战：数据结构化、模型参与和输出精炼。研究使用了一个新的开源布局感知IE测试套件LayIE-LLM，并与传统微调过的IE模型进行了基准测试。
### Innovation
提出了一个针对布局丰富的文档中的信息提取问题的测试套件LayIE-LLM，该套件用于评估和优化IE管道的设计选择。通过这种方法，发现优化配置比通用实践基准配置的F1分数提高了13.3-37.5点。此外，开发了一种逐步单因分析（OFAT）方法，该方法以较低的计算成本获得了接近最优的结果。
### Conclusion
展示了如果适当配置，通用大型语言模型可以与专门模型媲美的性能，提供了一种成本效益高且无需微调的可替代方案。这种测试套件对于布局丰富的文档的信息提取是一个有价值的工具。
## 26. `cs.CL` - 通过最佳和最差令牌实现奖励模型可解释性 [PDF](https://arxiv.org/pdf/2506.07326), [HTML](https://arxiv.org/abs/2506.07326)
### Authors
Brian Christian,Hannah Rose Kirk,Jessica A.F. Thompson,Christopher Summerfield,Tsvetomira Dumbalska
### Background
奖励建模已成为使大型语言模型与人类价值观对齐的关键组成部分。尽管对通过奖励模型进行微调生成模型的注意较多，但直接将人类价值判断编码为提示-响应对的奖励分数的奖励模型本身仍然相对研究较少。
### Innovation
该研究提出了一种新颖的方法，通过全面分析奖励模型在其整个词汇空间中的响应来提高其可解释性。通过评估每个可能的单个令牌对有价值观提示的评分，发现了几个显著的发现，包括不同目标训练的模型之间的显著异质性、模型编码高分和低分令牌的系统不对称性以及对提示框架的高度敏感性，这反映了人类的认知偏见。
### Conclusion
研究结果挑战了奖励模型可互换性的假设以及它们作为复杂和上下文相关的人类价值观代理的适用性。研究发现，这些模型可能会对某些身份群体产生有偏见的看法，这些偏见可能是在无害训练中无意产生的，存在传播到当前已部署给数百万用户的下游大型语言模型的风险。
## 27. `cs.CL` - 破碎的令牌？你的语言模型可以秘密处理非规范令牌化。 [PDF](https://arxiv.org/pdf/2506.19004), [HTML](https://arxiv.org/abs/2506.19004)
### Authors
Brian Siyuan Zheng,Alisa Liu,Orevaoghene Ahia,Jonathan Hayase,Yejin Choi,Noah A. Smith
### Background
现代词分割器使用确定性算法将文本映射为单一的“规范”令牌序列，但同一个字符串可以用词分割器词汇中的多种非规范词分割方法编码。本文研究了模型在遇到完全未在训练期间见过的非规范词分割时的鲁棒性。
### Innovation
研究发现指令调优模型在随机词分割或字符级别词分割时，仍能保持93.4%和90.8%的原始性能。此外，非规范词分割方案在某些情况下可以提升性能，如字符级别分隔增强字符串操作和代码理解任务可达14%，右侧对齐的数字分组增强大规模数字计算可达33%。研究还发现这种鲁棒性源于指令调优阶段，模型能理解非规范词分割，但基础模型倾向于模仿想象中的错误并产生非有意义的输出，而调优后模型则坚持产出流畅的回答。
### Conclusion
研究结果表明模型比以往认为的更加独立于其词分割器，并展示了在推理阶段介入词分割以提高性能的潜力。
## 28. `cs.CL` - 结构对齐：使大型语言模型与结构信息对齐 [PDF](https://arxiv.org/pdf/2504.03622), [HTML](https://arxiv.org/abs/2504.03622)
### Authors
Zae Myung Kim,Anand Ramachandran,Farideh Tavazoee,Joo-Kyung Kim,Oleg Rokhlenko,Dongyeop Kang
### Background
大型语言模型（LLMs）在生成长篇连贯文本方面仍面临挑战，因为它们缺乏层次规划和论据生成的结构组织。现有的模型难以生成具有高度组织性和连贯性的长文本。
### Innovation
本文提出了一种名为Structural Alignment的新方法，旨在通过将语言论据框架整合到强化学习中，将LLMs与类人论据结构对齐，从而提升长文本生成的质量。使用密集奖励方案，在Proximal Policy Optimization框架内，基于论据相对人类写作的独特性分配细粒度的token级奖励。评估了两种互补的奖励模型：一个通过表面文本特征提高可读性来提供明确的结构引导，另一个通过分析多层次论据模式加强深层次的连贯性和修辞复杂性。该方法在作文生成和长文档摘要等任务上优于标准模型和RLHF增强模型。
### Conclusion
本文通过Structural Alignment方法改进了大型语言模型在长文本生成中的表现，该方法利用强化学习和语言学论据框架，有效提高了生成文本的连贯性和组织性，并在多个任务中优于现有方法。所有训练数据和代码将公开发布。
## 29. `cs.CL` - 基于较大规模变换器的语言模型的失误率预测fMRI数据效果较差 [PDF](https://arxiv.org/pdf/2506.11338), [HTML](https://arxiv.org/abs/2506.11338)
### Authors
Yi-Chien Lin,William Schuler
### Background
近期研究关注变压器为基础的语言模型（LMs）的意外信息量在人类句法处理难度预测上的应用。这些模型的每个单词的估计概率与其意外信息量预测阅读时间的能力成反比，这表明具有更多参数和更大数据集训练的模型对人类阅读时间的预测能力较弱。之前的研究所采用的较为小型的语言模型测试，在脑部成像数据上的表现没有明显趋势，这提示我们反比现象可能只局限于时滞指标。因此，这项研究通过17个预训练LMs在三种不同的LM家族上在两个功能性磁共振成像(fMRI)数据集上进行了更全面的评估。
### Innovation
研究使用了17个预训练LMs在三种不同的LM家族上对两个功能性磁共振成像(fMRI)数据集进行了评估，发现模型的每个单词的估计概率与模型在两个数据集上的拟合度之间存在反比关系，这是对之前研究结果的确认，表明这一趋势不仅限于时滞指标，还扩展到了神经反应数据。
### Conclusion
研究结果表明，具有更多参数和更大数据集训练的模型对人类阅读时间的预测能力较低，且这种反比现象不仅限于时滞指标，也适用于神经反应数据。
## 30. `cs.LG` - 抗反淘汰指纹识别 [PDF](https://arxiv.org/pdf/2602.03812), [HTML](https://arxiv.org/abs/2602.03812)
### Authors
Yixuan Even Xu,John Kirchenbauer,Yash Savani,Asher Trockman,Alexander Robey,Tom Goldstein,Fei Fang,J. Zico Kolter
### Background
模型蒸馏能够高效模仿前沿大规模语言模型（LLMs），因此需要稳健的机制来检测第三方学生模型是否曾基于教师模型的输出进行训练。现有用于检测此类蒸馏的指纹技术基于启发式扰动，通常需要在生成质量和指纹强度之间做出陡峭的权衡，可能导致学生模型的实用性显著下降。
### Innovation
提出了一种名为抗反淘汰指纹识别（ADFP）的方法，这是一种原理性的方法，将指纹识别目标与学生模型的学习动态相一致。ADFP基于抗反淘汰采样的梯度基础框架，利用代理模型来识别和采样能够直接最大化学生模型在微调后检测指纹预期可检测性的令牌，而非依赖于吸收未针对的更简单水印的无目标偏差。
### Conclusion
在GSM8K和OASST1基准测试上的实验表明，ADFP在保持最小实用性影响的情况下，广泛优于最先进的基线方法，提供了更强的检测信心，即使学生模型的架构未知也是如此。
## 31. `cs.LG` - 在那使用合成数据？合成数据在数据共享和增强中的适用性分析 [PDF](https://arxiv.org/pdf/2602.03791), [HTML](https://arxiv.org/abs/2602.03791)
### Authors
Bogdan Kulynych,Theresa Stadler,Jean Louis Raisaro,Carmela Troncoso
### Background
近年来生成模型的发展使得合成数据被广泛认为是解决数据访问、稀缺性和代表性不足问题的一种有效方法。本文探讨了合成数据的三个主要应用场景：（1）通过代理私有数据集中的合成数据来实现统计分析同时保护隐私；（2）通过增加合成数据来提升机器学习模型的性能；（3）通过增加合成数据来减少统计估计的方差。本文通过正式分析和案例研究，探讨了每种应用场景下合成数据是否能实现其预期目标，并识别出合成数据在特定问题上的固有问题和实用限制。
### Innovation
本文对合成数据的三个主要应用场景进行了正式分析和案例研究，揭示了由于这些限制，许多现有的或设想中的合成数据使用场景都与问题的匹配度不佳。提供的正式定义和合成数据使用案例分类有助于决策者评估特定数据可用性问题是否适合采用合成数据解决方案。
### Conclusion
本文的分析表明，由于固有问题和实用限制，许多现有或设想中的合成数据使用场景并不适合具体的问题。提供的正式定义和合成数据使用案例分类可以帮助决策者评估合成数据是否是解决他们特定数据可用性问题的合适方法。
## 32. `cs.LG` - 桥接在线与离线强化学习：情境定向臂学习在多轮代码生成中的应用 [PDF](https://arxiv.org/pdf/2602.03806), [HTML](https://arxiv.org/abs/2602.03806)
### Authors
Ziru Chen,Dongdong Chen,Ruinan Jin,Yingbin Liang,Yujia Xie,Huan Sun
### Background
近年来，研究人员对使用强化学习（RL）训练大规模语言模型（LLMs）产生了浓厚兴趣，特别是在进行多轮代码生成等实际任务时。在线RL通常表现更好，但其高昂的训练成本和不稳定性限制了其广泛应用。
### Innovation
本文通过将多轮代码生成问题形式化为一步可恢复的马尔可夫决策过程，提出了结合在线与离线RL优势的新方法——情境定向臂学习（Cobalt）。Cobalt首先使用参考的LLM收集代码生成轨迹，并将其分解为部分轨迹作为情境提示；然后，在线臂学习阶段，通过单步代码生成训练LLM来完成每个部分轨迹提示。Cobalt在LiveCodeBench上大幅提升了R1-Distill 8B和Qwen3 8B的Pass@1得分，分别提高9.0和6.2分，并分析并缓解了LLMs的上下文奖励劫持行为。
### Conclusion
实验结果证明，Cobalt在迭代决策任务如多轮代码生成方面是非常有潜力的解决方案。
## 33. `cs.LG` - SymPlex: 一种用于符号偏微分方程求解的结构感知Transformer [PDF](https://arxiv.org/pdf/2602.03816), [HTML](https://arxiv.org/abs/2602.03816)
### Authors
Yesom Park,Annie C. Lu,Shao-Ching Huang,Qiyang Hu,Y. Sungtaek Ju,Stanley Osher
### Background
传统方法在求解偏微分方程（PDEs）时大多依赖于数值算法或神经网络，这些方法要么在离散空间中近似解，要么在隐函数空间中寻找解，无法直接在符号表达空间中提供解析解。因此，本文旨在提出一种新的框架来自动发现符号的解析解，即SymPlex。
### Innovation
SymPlex是一个基于强化学习的符号解发现框架，通过将其建模为具有树结构决策过程，并仅使用PDE及其边界条件来优化候选解。它的核心是SymFormer，这是一种结构感知Transformer，利用树相关自注意力模型层次结构中的符号依赖关系，并通过句法约束自回归解码确保语法有效，克服了基于序列的生成器的表达能力限制。与传统的数值和神经逼近方法不同，SymPlex直接在符号表达空间中操作，提供了可解释且人类易读的解，自然地表示非光滑行为和显式参数依赖。
### Conclusion
实验证明，使用基于深度学习的符号方法，SymPlex能够精确恢复非光滑和参数化的PDE解。
## 34. `cs.LG` - 使用CTF中基于管子的混合机器学习模型预测棒束中的临界热流 [PDF](https://arxiv.org/pdf/2602.03805), [HTML](https://arxiv.org/abs/2602.03805)
### Authors
Aidan Furlong,Robert Salko,Xingang Zhao,Xu Wu
### Background
近年来，使用机器学习（ML）方法预测临界热流（CHF）已成为一个非常重要且活跃的研究领域。其目标是建立比现有工程关联式或查表法更准确的模型。虽然之前的研究已经开发并部署了基于管子的纯和混合ML模型于CTF子通道代码中，但在全尺寸反应堆核心模拟中，需要使用棒束几何结构。与孤立子通道相比，棒束会经历复杂的热液压现象，如通道侧流、栅格损失以及未加热元件的影响。
### Innovation
这项研究的重点在于，在基于管子的CHF数据训练后，将ML基预测CHF模型推广应用于棒束几何结构中。通过CTF子通道代码实现了一个全数据驱动的DNN模型，并且开发了两个具有偏置校正功能的混合模型，它们被用来预测Combustion Engineering 5-by-5棒束的CHF位置和规模。以W-3关联、Bowring关联以及Groeneveld查表法作为基准对比模型。
### Conclusion
所有三种基于ML的方法产生的CHF预测结果在幅度和位置上都优于基准模型，其中基于查表法的混合模型表现出最好的性能指标。
## 35. `cs.LG` - Manifold Random Features [PDF](https://arxiv.org/pdf/2602.03797), [HTML](https://arxiv.org/abs/2602.03797)
### Authors
Ananya Parashar,Derek Long,Dwaipayan Saha,Krzysztof Choromanski
### Background
本文提出了一个用于在通用流形上近似二元函数（特别是核函数）的新范式。新的机制结合了流形的离散化和最近引入的图随机特征（GRFs）技术，用于在流形上学习连续场。这些场用于找到其他情况下无法通过解析方法获取的连续近似机制。
### Innovation
本文提出了一种新的机制，Manifold Random Features (MRFs)，它利用了流形的离散化和图随机特征（GRFs）技术，用于在流形上近似和学习连续场，这些场可以有效地逼近复杂的函数。该机制还重新发现了最近引入的高斯核近似机制，并通过图上的简单随机游走进行优化，绕过了复杂的数学计算。
### Conclusion
本研究展示了图随机特征和连续随机特征之间深层次的渐近联系，并通过严格的理论分析和彻底的实验验证来补充算法。
## 36. `cs.LG` - 从紧急停车干预中学习鲁棒干预 [PDF](https://arxiv.org/pdf/2602.03825), [HTML](https://arxiv.org/abs/2602.03825)
### Authors
Ethan Pronovost,Khimya Khetarpal,Siddhartha Srinivasa
### Background
在自主系统测试过程中，人类干预是常见的数据来源。这些干预为改进当前策略提供了重要信号，但这些信号通常具有噪音且不完整。研究如何在干预数据质量不高且反馈不完全的情况下学习，成为了一个研究热点。
### Innovation
提出了鲁棒干预学习（RIL）的概念，旨在学习来自干预数据的同时，对干预信号的质量和信息性保持鲁棒性。针对紧急停车干预问题，提出了残差精调细调算法（RIFT），将干预反馈视为不完整的学习信号，并将其与先验策略显式结合。通过将干预学习视为精调问题，我们的方法利用先验策略中编码的结构来解决干预信号指定不充分时的不确定性。
### Conclusion
实验结果表明，残差精调细调算法在不同干预策略和先验策略质量下能实现鲁棒且一致的策略改进。这表明鲁棒干预学习是未来研究的一个很有前景的方向。
## 37. `cs.LG` - 使用同伦预测的推理时间去学习 [PDF](https://arxiv.org/pdf/2602.03787), [HTML](https://arxiv.org/abs/2602.03787)
### Authors
Somnath Basu Roy Chowdhury,Rahul Kidambi,Avinava Dubey,David Wang,Gokhan Mergen,Amr Ahmed,Aranyak Mehta
### Background
机器遗忘是指有效地从训练好的机器学习模型中移除特定信息的过程，而不需要从头开始重新训练。现有的遗忘方法往往提供了可证明的保证，这些方法通常是通过基于遗忘集合重新训练模型的一部分参数来实现的。尽管这些方法在某些场景中显示出前景，但在实际应用中，尤其是在应用到生成模型时，它们的假设常被挑战。此外，使用这些遗忘过程来更新参数往往损害了模型在预训练期间获得的通用能力。
### Innovation
本文提出了一个框架，该框架在推理阶段利用验证器来评估模型响应是否满足适当的遗忘保证，并在此基础上迭代地改进生成响应的质量，而无需更新模型参数。该框架利用了同伦预测来减少计算开销并提供无分布假设的遗忘保证。这种方法在多种挑战性的遗忘基准上显著优于现有最先进的方法，将遗忘错误降低了高达93%。
### Conclusion
该论文提出的方法在多个挑战性的去学习基准测试中显著优于现有的最先进的方法，通过使用同伦预测，降低了高达93%的去学习错误，同时证明了通用保证，这些保证不需要更新模型参数即可提高生成响应的质量。
## 38. `cs.LG` - 通过课程指导的特征学习和三阶段注意力网络提升不平衡节点分类 [PDF](https://arxiv.org/pdf/2602.03808), [HTML](https://arxiv.org/abs/2602.03808)
### Authors
Abdul Joseph Fofanah,Lian Wen,David Chen,Shaoyang Zhang
### Background
在图神经网络（GNNs）中，当某些标签比其他标签更为常见时，会发生不平衡节点分类问题，这会导致模型对少数类别的表现不佳。这种问题导致模型学习不公，性能较差。本研究旨在解决这个问题。
### Innovation
提出了一个名为CL3AN-GNN的课程指导的特征学习和三阶段注意力网络。该模型采用了一种类似人类学习过程的三步注意力系统（Engage, Enact, Embed），首先通过结构上简单的特征进行学习，然后处理复杂的特征，最后通过逐迭代消息传递和课程对齐的损失权重化进行汇总。此方法在八个包含社交、生物学和引用网络的开放图基准数据集上进行评估，结果显示在准确性、F1分数和AUC方面的一致改进。而且，此方法适用于不同类型的图数据集，比一次性训练所有内容更有效，能更好地处理新、不平衡的图，并且每一步都有清晰的解释，证明了课程学习框架的有效性。
### Conclusion
本文提供了一个课程学习在GNNs中的理论框架，并通过实验证明了其有效性和对不平衡的强大抵抗力，通过指标、收敛速度和泛化测试进行验证。
## 39. `cs.LG` - 用于任务归因的高效内核代理模型估计 [PDF](https://arxiv.org/pdf/2602.03783), [HTML](https://arxiv.org/abs/2602.03783)
### Authors
Zhenshuo Zhang,Minxuan Duan,Hongyang R. Zhang
### Background
现代AI代理（如大型语言模型）同时训练多种多样任务，如翻译、代码生成、数学推断和文本预测。研究关键问题是如何量化各个训练任务对目标任务表现的影响，即任务归因问题。直接方法是通过对每个任务进行leave-one-out重新训练来测量影响，但这种方法在大规模下是不可行的。为了解决这一限制，近期文献提出了构建代理模型的方法，旨在预测目标任务在任何设定训练任务子集时的表现。此前的研究主要集中在线性代理模型上，但它们忽略了非线性交互如协同效应、对抗效应或XOR类型的交互。
### Innovation
1. 提出了一种统一的任务权重框架来分析任务归因方法，并通过二次分析展示了线性代理模型和影响函数的新联系。2. 引入了内核代理模型，能够更好地表示二次任务交互。3. 开发了一种基于梯度的估计程序，利用预训练模型的一阶近似来学习内核代理模型，从而在相对误差小于2%的情况下获得准确估计，而无需重复重新训练。4. 实验结果表明，内核代理模型在多个领域（如转录器中的数学推理、上下文学习和多目标强化学习）的表现优于线性代理模型和影响函数基准，与leave-one-out基准的关联性高出25%，用于下游任务选择时在上下文学习和多目标强化学习基准上表现出40%的改进。
### Conclusion
这项研究通过构建内核代理模型，有效解决了任务归因的非线性交互问题，并通过实验结果展示了其优越性，为任务归因方法提供了新的视角和技术支持。
