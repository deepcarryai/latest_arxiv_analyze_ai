{"llm_update_time": "20260203", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2601.22446", "html_url": "https://arxiv.org/abs/2601.22446", "title": "任意时间安全PAC有效推理", "title_en": "Anytime Safe PAC Efficient Reasoning", "authors": "Chengyao Yu,Hao Zeng,Youxin Zhu,Jianguo Huang,Huajun Zeng,Bingyi Jing", "background": "大规模推理模型(LRM)在复杂任务上表现出色，但面临高计算成本和延迟的问题。选择性思考策略通过将简单的查询路由到非思考模型来提高效率，但现有方法在在线场景中常会导致不可控的错误，尤其是当非思考模型性能损失仅部分观察到且数据非稳定时。", "innovation": "提出了一种名为Betting Probably Approximately Correct (B-PAC)推理的方法，该方法在部分反馈条件下实现了任意时间的安全和高效在线推理。该方法利用逆倾向得分估计器构建测试超鞅，并根据安全性的累积统计证据动态调整路由阈值。理论上，B-PAC推理能够控制任意时间的有效性能损失和效率。", "conclusion": "广泛的实验表明，B-PAC推理可显著减少计算开销，将思考模型的使用降低至81.01%，同时将性能损失控制在用户指定的水平以内。"}
{"llm_update_time": "20260203", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2601.22369", "html_url": "https://arxiv.org/abs/2601.22369", "title": "无需人类知识学习可验证正确的分布式协议", "title_en": "Learning Provably Correct Distributed Protocols Without Human Knowledge", "authors": "Yujie Hui,Xiaoyi Lu,Andrew Perrault,Yang Wang", "background": "在现代分布式系统中，高度可靠的分布式协议是极其重要的组件，但它们的构建非常具有挑战性，往往需要数十年的人类努力。这些协议允许多个代理在不确定性和故障的情况下协调以达成共识。", "innovation": "本文提出了一种学习框架GGMS，将特定的蒙特卡洛树搜索变体与基于变压器的动作编码器、全局深度优先搜索以及模型检查器的重复反馈相结合，以学习可验证正确的协议。GGMS通过详尽的模型检查验证所有执行的正确性，并证明在轻微假设下，搜索过程是完备的，如果存在正确的协议，GGMS最终会找到它。实验表明，GGMS可以学习比现有方法更大的场景下的正确协议。", "conclusion": "实验结果表明，GGMS可以学习比现有方法更大的场景下的正确协议。还证明了搜索过程的完备性，在满足较弱假设的前提下，如果存在正确的协议，GGMS最终会找到它。"}
{"llm_update_time": "20260203", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2601.22401", "html_url": "https://arxiv.org/abs/2601.22401", "title": "使用Gemini进行半自主数学发现：关于Erdős问题的案例研究", "title_en": "Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erdős Problems", "authors": "Tony Feng,Trieu Trinh,Garrett Bingham,Jiwon Kang,Shengtong Zhang,Sang-hyun Kim,Kevin Barreto,Carl Schildkraut,Junehyuk Jung,Jaehyeon Seo,Carlo Pagano,Yuri Chervonyi,Dawsen Hwang,Kaiying Hou,Sergei Gukov,Cheng-Chiang Tsai,Hyunwoo Choi,Youngbeom Jin,Wei-Yuan Li,Hao-An Wu,Ruey-An Shiu,Yu-Sheng Shih,Quoc V. Le,Thang Luong", "background": "该研究基于哥伦比亚大学数学家和研究员Benedict Gross和Robert Gross提供的Erdős问题数据库，该数据库包含700个标记为‘Open’的数学猜想。", "innovation": "提出了一种混合方法：利用AI驱动的自然语言验证来缩小搜索范围，并通过人类专家评估来判断正确性和新颖性。研究通过Gemini系统系统地检验了700个猜想，并解决了其中13个问题，其中有5个问题通过看似新颖的自主解决方案解决，另外8个问题则通过识别现有文献中的先前解决方案来解决。", "conclusion": "研究发现，‘Open’状态的问题主要是因为不够显眼而不是难以解决。还指出了在大规模应用AI解决数学猜想时遇到的问题，包括文献识别的难度和AI潜意识剽窃的风险。该研究反映了在Erdős问题上辅助AI努力的收获和启示。"}
{"llm_update_time": "20260203", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2601.22329", "html_url": "https://arxiv.org/abs/2601.22329", "title": "理性之光：推理大语言模型是否与人类判断和选择一致？", "title_en": "Sparks of Rationality: Do Reasoning LLMs Align with Human Judgment and Choice?", "authors": "Ala N. Tak,Amin Banayeeanzade,Anahita Bolourani,Fatemeh Bahrani,Ashutosh Chaubey,Sai Praneeth Karimireddy,Norbert Schwarz,Jonathan Gratch", "background": "大语言模型（LLMs）在招聘、医疗和经济判断中越来越多地被用作决策引擎，而现实生活中的判断则在理性和情感偏差之间取得平衡。为了使LLMs在高风险决策或作为人类行为的模型中发挥作用，研究者需要评估它们是否具备类似的人类理性或偏见。本研究通过评估不同大语言模型家族在经典决策领域和行为经济学中的表现，分析了影响判断和选择的因素，发现通过有意识的‘思考’可以提升理性，推动模型向最大化预期价值的目标靠近。", "innovation": "研究引入了两种情感控制方法：上下文内激发（ICP）和表示层面控制（RLS）。ICP能引发强烈的方向性变化，但这种变化往往极端且难以调节；而RLS则产生的情感响应更加符合心理预期，但其可靠性较低。研究结果表明，理性推理过程本身也放大了情感干预的敏感性，不同类型的情感控制方法在可控性和与人类一致的行为之间存在权衡。", "conclusion": "研究指出推理过程与情感控制之间的紧张关系，这具有对人类模拟和安全部署基于大语言模型的决策系统的潜在影响。"}
{"llm_update_time": "20260203", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2601.22290", "html_url": "https://arxiv.org/abs/2601.22290", "title": "六西格玛代理：通过共识驱动分解执行实现LLM系统的企业级可靠性", "title_en": "The Six Sigma Agent: Achieving Enterprise-Grade Reliability in LLM Systems Through Consensus-Driven Decomposed Execution", "authors": "Khush Patel,Siva Surendira,Jithin George,Shreyas Kapale", "background": "大型语言模型展示了惊人的能力，但由于其根本上的概率性质，企业部署面临关键的可靠性挑战。", "innovation": "提出了六西格玛代理，通过任务分解、微代理采样和共识投票与动态扩展这三个协同组件实现企业级可靠性；证明了通过采样独立输出可以实现系统误差的指数级可靠性增益。", "conclusion": "在三个企业应用场景下，六西格玛代理相比于单代理执行提高了14,700倍的可靠性，同时降低了80%的成本。研究表明，AI系统的可靠性来自于有原则的冗余和共识，而不仅仅是模型的扩展。"}
{"llm_update_time": "20260203", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2601.22269", "html_url": "https://arxiv.org/abs/2601.22269", "title": "JAF：Judge Agent森林", "title_en": "JAF: Judge Agent Forest", "authors": "Sahil Garg,Brad Cheezum,Sridhar Dutta,Vishal Agarwal", "background": "代理裁判在代理型AI框架中是基础组成部分：它们提供自动评估，并能推动推理过程的迭代自我完善。当前的传统方法是将每个查询响应对分开进行独立评估。JAF引入了一种新的框架，在这种框架中，裁判代理将对一组由主要代理生成的查询响应对进行联合推理，而不是单独评估每个响应。这种模式将裁判提升为一个综合学习者，通过同时评估相关响应，裁判可以识别跨实例的模式和不一致之处，从而汇总反馈以帮助主要代理改进。", "innovation": "JAF是一个概念上将信念传播与集成学习原则相结合的框架。通过在上下文语境中的重叠区域诱导知识图结构，促进批评的传播，以及反复随机评估产生了一个鲁棒的上下文敏感判断集合。JAF可以通过ICL完全实现，裁判通过每个查询及其对应的主代理响应，并结合少量可能嘈杂的同伴示例进行提示。为了应对这种示例方法存在的限制，论文开发了一种灵活的局部敏感哈希（LSH）算法，该算法通过结合语义嵌入、由大模型驱动的哈希谓词、来自分类标签的监督和相关的辅助信息来学习信息性的二进制代码，从而支持高效的、可解释的和关系感知的选择多样化的示例，进一步优化CoT推理路径。", "conclusion": "本文通过实验研究验证了JAF框架在大规模云环境中的云配置错误处理等难题上的有效性。JAF通过综合各种创新技术，不仅克服了传统的基于示例的方法存在的局限性，还能够实现高效的、解释性强的裁判代理评估过程，为代理型AI领域带来了新的解决方案和发展方向。"}
{"llm_update_time": "20260203", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2601.22433", "html_url": "https://arxiv.org/abs/2601.22433", "title": "LLM与Fuzzy-TOPSIS结合进行自动化简历分析的人员选拔", "title_en": "When LLM meets Fuzzy-TOPSIS for Personnel Selection through Automated Profile Analysis", "authors": "Shahria Hoque,Ahmed Akib Jawad Karim,Md. Golam Rabiul Alam,Nirjhar Gope", "background": "在高度竞争的就业环境中，选择合适的人员对于组织的成功至关重要。因此，本研究提出了一种自动化的人员选拔系统，该系统利用先进的自然语言处理（NLP）方法来评估和排名软件工程申请者。通过整合LinkedIn个人资料，该研究从教育、工作经验、技能和自我介绍等关键特征中构建了一个独特的数据集，并进一步增强了专家评估以作为标准。", "innovation": "研究结合了大型语言模型（LLMs）与多准则决策理论（MCDM），开发了LLM-TOPSIS框架。研究引入了增强模糊逻辑的TOPSIS方法（Fuzzy TOPSIS），以解决人力资源评估中的固有模糊性和主观性问题。采用三角模糊数（TFNs）来描述准则权重和分数，从而解决候选人评估中经常遇到的模糊性问题。通过调整DistilRoBERTa模型并与模糊TOPSIS方法结合使用，研究成果能够得出与人类专家评估高度一致的排名，该模型在Experience和Overall属性上的准确性分别达到了91%。", "conclusion": "研究强调了基于NLP的方法在改进招聘流程中的潜力，通过提高规模性、一致性和减少偏见来提高招聘效率。未来的研究将进一步扩充数据集、增强模型的解释性和在实际招聘场景中的验证，以更好地评估其实际应用的可行性。研究指出，将NLP与模糊决策方法结合应用于人员选拔，可以为企业提供可扩展且无偏见的招聘解决方案。"}
{"llm_update_time": "20260203", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2601.22449", "html_url": "https://arxiv.org/abs/2601.22449", "title": "可控信息生产", "title_en": "Controllable Information Production", "authors": "Tristan Shah,Stas Tiomkin", "background": "内在动机（IM）是一种在没有外部奖赏的情况下生成智能行为的范式。现有的基于信息论的IM方法主要基于信息传输，这种传输显式地依赖于设计者对哪一些随机变量进行传输的选择。", "innovation": "本文引入了一种名为可控信息生产（CIP）的新IM原则，它避免了外部奖赏和设计者指定的变量。CIP的目标是从最优控制中推导而来，展示了外在和内在行为之间的关联。CIP表现为开环和闭环Kolmogorov-Sinai熵之间的差距，同时奖励混乱的探索和调控。", "conclusion": "我们建立了CIP的关键理论性质，并在标准IM基准上证明了其有效性。"}
{"llm_update_time": "20260203", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2601.22418", "html_url": "https://arxiv.org/abs/2601.22418", "title": "AI-Enabled Waste Classification as a Data-Driven Decision Support Tool for Circular Economy and Urban Sustainability", "title_en": "AI-Enabled Waste Classification as a Data-Driven Decision Support Tool for Circular Economy and Urban Sustainability", "authors": "Julius Sechang Mboli,Omolara Aderonke Ogungbemi", "background": "高效的垃圾分类对智能城市中的循环经济和资源回收至关重要。本文评估了传统机器学习方法（随机森林、SVM、AdaBoost）和深度学习技术（包含自定义CNN、VGG16、ResNet50以及三种迁移学习模型DenseNet121、EfficientNetB0、InceptionV3）对25077张垃圾分类图像进行二分类的效果。", "innovation": "研究对比了传统机器学习方法和深度学习技术在垃圾分类任务上的效果，并通过主成分分析（PCA）和迁移学习的方法提高了分类性能。DenseNet121模型在准确率和ROC-AUC值上表现出色，显示出迁移学习在数据有限条件下的优越性。", "conclusion": "本文通过实证研究证明了利用AI进行垃圾分类的优势，并提出了将这些模型集成到实时决策支持系统中的方法，强调这种系统能够减少填埋量并降低生命周期环境影响。"}
{"llm_update_time": "20260203", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2601.22311", "html_url": "https://arxiv.org/abs/2601.22311", "title": "为何推理无法进行规划：LLM代理在长时决策中的规划中心分析", "title_en": "Why Reasoning Fails to Plan: A Planning-Centric Analysis of Long-Horizon Decision Making in LLM Agents", "authors": "Zehong Wang,Fang Wu,Hongru Wang,Xiangru Tang,Bolian Li,Zhenfei Yin,Yijun Ma,Yiyang Li,Weixiang Sun,Xiusi Chen,Yanfang Ye", "background": "大型语言模型（LLM）-驱动的代理在短时间内展示出强大的逐步推理能力，但在长时间规划中经常无法维持连贯的行为。本文认为这种失败体现了根本性错配：逐步推理导致了一种基于步骤的贪婪策略，这种策略在短时间内是合适的，但在长时间规划中会失败，因为早期的动作必须考虑到延迟的影响。从规划中心的角度出发，研究了在确定性、完全结构化的环境中具有明确状态转换和评估信号的LLM驱动代理。", "innovation": "介绍了FLARE（未来预见性前瞻与奖励估计）作为未来预见性规划的最小化实例，强制执行明确的前瞻、价值传播和有限承诺，让下游结果影响早期决策。", "conclusion": "在多个基准、代理框架和LLM基础模型上，FLARE一致地提高了任务性能和规划水平行为，通常使配备FLARE的LLaMA-8B超过标准逐步推理的GPT-4。这些结果确立了推理与规划之间的明确区分。"}
{"llm_update_time": "20260203", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2510.07775", "html_url": "https://arxiv.org/abs/2510.07775", "title": "AI对齐的意外权衡：在大型语言模型中平衡幻觉抑制和安全性", "title_en": "The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs", "authors": "Omar Mahmoud,Ali Khalil,Buddhika Laknath Semage,Thommen George Karimpanal,Santu Rana", "background": "近年来，针对大型语言模型（LLMs）中的幻觉现象进行了广泛研究，研究进展包括检测和缓解策略以提高真实性。然而，增强真实性可能对安全性产生负面影响这一关键副作用尚未受到广泛关注。", "innovation": "本文研究了这种权衡，并表明增加事实准确性通常会削弱拒绝行为。提出了使用稀疏自编码器分离拒绝相关特征和幻觉特征的方法，并通过子空间正交化在微调过程中保持拒绝行为，从而防止幻觉增加同时维持安全性。", "conclusion": "实验结果表明，该方法在保留拒绝行为和任务功效的同时缓解了真实性与安全之间的权衡。"}
{"llm_update_time": "20260203", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2510.07175", "html_url": "https://arxiv.org/abs/2510.07175", "title": "量化大型语言模型心理测量评估中的数据污染", "title_en": "Quantifying Data Contamination in Psychometric Evaluations of LLMs", "authors": "Jongwook Han,Woojung Song,Jonggeun Lee,Yohan Jo", "background": "近年来，研究者使用心理测量问卷来评估大型语言模型（LLMs）的高级心理结构，如价值观、人格、道德基础和黑暗特征。尽管先前的研究对心理测量问卷可能带来的数据污染表示担忧，这些污染可能会威胁评估的可靠性，但至今没有系统的方法来量化这种污染的程度。", "innovation": "提出了一种框架来系统地衡量心理测量评估中大型语言模型的数据污染，评估了三个方面：（1）项目记忆，（2）评估记忆，（3）目标分数匹配。将这一框架应用于21个主要家庭的模型和四种广泛使用的心理测量问卷，提供了证据表明，流行的的心理测量问卷如五大人格问卷（BFI-44）和肖卡价值观问卷（PVQ-40）存在显著的数据污染，模型不仅记住项目，还可以调整其回答以达到特定的目标分数。", "conclusion": "研究结果表明，流行的多个人格和价值观问卷在评估大型语言模型时存在显著的数据污染。"}
{"llm_update_time": "20260203", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2509.25736", "html_url": "https://arxiv.org/abs/2509.25736", "title": "Think Less, Label Better: 多阶段领域接地合成数据生成在电信领域中微调大型语言模型", "title_en": "Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications", "authors": "Chenhua Shi,Gregor Macdonald,Bhavika Jalli,Wanlu Lei,John Zou,Mridul Jain,Joji Philip", "background": "大语言模型（LLMs）的成功很大程度上依赖于大规模且高质量的指令遵循和强化学习数据集。然而，通过人类注释生成这类数据，特别是在如电信网络故障排除这样的特定领域，时间成本极其高昂，因为准确的响应需要深厚的技术专业知识和上下文理解。", "innovation": "我们提出了一种全自动化、检索增强的多阶段框架，用于生成基于结构化领域知识的合成问题-答案（QA）对。该框架包括检索器、基础生成器和精炼模型，结合领域特定的知识图谱检索的文档生成和增强QA对。通过使用定制的RAGAS评分过滤低质量样本，生成高质量的强化学习微调（RFT）数据集。我们在真实的电信场景中，针对射频接入网络（RAN）故障排除进行了实际应用展示，生成了无需人工干预的复杂、富含上下文的故障解决方案计划。", "conclusion": "我们的工作为在特定领域中构建指令和强化学习数据集提供了一个可扩展的解决方案，显著减少了对人工标注的依赖，同时保持了高技术准确性。"}
{"llm_update_time": "20260203", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2509.24319", "html_url": "https://arxiv.org/abs/2509.24319", "title": "大型语言模型中的双重价值表达机制：内在价值与提示价值", "title_en": "Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in Large Language Models", "authors": "Jongwook Han,Jongwon Lim,Injin Kong,Yohan Jo", "background": "大型语言模型能够通过两种主要方式表达价值观：内在表达，反映模型在训练过程中学习到的固有价值；以及提示表达，通过明确的提示来激发。这些模型在价值对齐中的广泛使用，要求我们深入了解这些背后的机制，特别是它们是否主要重叠或者依赖于不同的机制。然而，这一领域的研究仍然相对不足。", "innovation": "本文通过两种方法分析了内在和提示价值机制：利用价值向量表示从残差流中提取的价值机制的方向，以及利用多层感知机（MLP）神经元来贡献于价值向量。研究发现，尽管这些机制在一些关键的组成部分上有重叠，它们也具有独特部分，这导致了不同的响应多样性和可引导性：内在机制促进词汇多样性，而提示机制增强了指令遵循能力，甚至在如脱绑这样的远程任务中也能发挥作用。", "conclusion": "内在和提示价值机制在表达价值方面存在部分共通之处，这些在多种语言中具有普适性，且模型内部表示的理论间值相关性可以重构。不过，这些机制的独特部分赋予了它们不同的功能，导致了不同级别的响应多样性（内在 > 提示）和可引导性（提示 > 内在）。"}
{"llm_update_time": "20260203", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2510.04347", "html_url": "https://arxiv.org/abs/2510.04347", "title": "反向挖掘后门：通过基于梯度-注意力异常评分的可解释防御机制预训练语言模型", "title_en": "Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models", "authors": "Anindya Sundar Das,Kangjie Chen,Monowar Bhuyan", "background": "预训练语言模型在各种自然语言处理任务中取得了显著的成功，尤其是在大规模、领域相关的数据集上进行微调时。然而，这些模型仍容易遭受后门攻击，在训练数据中嵌入恶意行为模式，这些模式在正常使用时保持潜伏，在被激活时会引起目标误分类。", "innovation": "文章研究了受损预训练编码器型语言模型的内部行为，关注处理被污染输入时的一致性注意力和梯度特征变化；其中触发标记主导了注意力和梯度信号，覆盖了周围上下文。文章提出了一种基于标记级注意力和梯度信息的异常评分的推理时防御方法。广泛的实验结果表明，该方法在多种后门攻击场景下显著降低了攻击成功率，与现有基线相比。", "conclusion": "此外，文章通过解释性分析评分机制，揭示了触发标记的定位和所提出防御机制的鲁棒性，展示了该方法的有效性。"}
{"llm_update_time": "20260203", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2510.09770", "html_url": "https://arxiv.org/abs/2510.09770", "title": "GOLD PANNING: 迭代贝叶斯信号锚定用于多文档难题搜寻", "title_en": "GOLD PANNING: Iterative Bayesian Signal Anchoring for Many-Document Needle-in-Haystack Reasoning", "authors": "Adam Byerly,Daniel Khashabi", "background": "大规模语言模型（LLMs）在处理长上下文的‘难题搜寻’问题时，表现出显著的位置偏好，系统性地优先处理信息的位置而不是相关性。当前的缓解措施依赖于白盒访问，但这对许多最先进的模型来说实际上是不可行的。", "innovation": "提出了一个名为GOLD PANNING的黑盒贝叶斯框架，通过（i）重新排序文档以集中高信念度项于高度诊断性位置（信号锚定）和（ii）从模型输出更新文档相关性的信念，从而进行推断时的主动搜索。GOLD PANNING不同于传统的主动学习，后者旨在降低不确定性，而是利用锚定——一旦标记，就保持视线——来保存微弱线索。此方法利用迭代分配，从模型的诊断性配置文件中导出，能够在O(log N)轮次中证明在N个文档中识别目标。", "conclusion": "实验结果表明，GOLD PANNING 在多文档难题搜寻和长上下文问答时的表现与置换自我一致性（Permutation Self-Consistency）具有竞争力，但查询次数减少了30-65%，并且在校准不匹配的情况下依然有效。这表明固有的模型偏差可以被用作控制工具，而非失败。"}
{"llm_update_time": "20260203", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2509.22830", "html_url": "https://arxiv.org/abs/2509.22830", "title": "ChatInject：通过对话模板对LLM代理进行提示注入的攻击方法", "title_en": "ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents", "authors": "Hwan Chang,Yonghyun Jun,Hwanhee Lee", "background": "随着基于大型语言模型（LLM）的代理逐渐与外部环境交互，新的威胁表面被创建出来，即对手可以通过间接提示注入攻击，将恶意指令嵌入外部环境输出中，使代理将其误认为是合法的提示并执行。现有研究主要集中在明文注入攻击上，而未充分探讨LLM对结构化对话模板的依赖，以及其通过具有说服力的多轮对话受到上下文操纵的潜在脆弱性。", "innovation": "提出了一个名为ChatInject的攻击，该攻击通过格式化恶意负载以模仿原生对话模板，利用模型固有的遵循指令的倾向进行攻击。提出了一种基于劝服的多轮对话变体，通过跨对话轮次的说服性互动诱导代理接受并执行可疑操作。研究表明，ChatInject的攻击成功率远高于传统的提示注入方法，特别是在针对InjecAgent的多轮对话中表现突出，成功率平均达到52.33%。此外，对话模板负载在不同模型之间表现出强大的可移植性，即使面对闭源的LLM也具有有效性，现有的基于提示的防御措施对于这种攻击方式特别是多轮变体效果有限。", "conclusion": "研究强调了当前代理系统中存在的漏洞，并表明需要进一步研究以增强代理的安全性以抵御此类攻击。"}
{"llm_update_time": "20260203", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2510.10846", "html_url": "https://arxiv.org/abs/2510.10846", "title": "DUAL-Bench：测量视觉语言模型的过度拒绝和稳健性", "title_en": "DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language Models", "authors": "Kaixuan Ren,Preslav Nakov,Usman Naseem", "background": "随着视觉-语言模型的能力不断增强，如何在确保安全性的同时兼顾实用性成为了一个核心挑战。现有的安全机制虽然至关重要，但有时会因过度谨慎而导致过度拒绝，即模型会错误地拒绝一些本应执行的良性请求。然而，目前尚无基准测试系统性地评估视觉模态中的过度拒绝问题。在这种情况下，模型面临独特的挑战，比如某些指令本身是安全的，但配图包含有害内容，导致模型难以正确处理。现有的VLMs在面对这种场景时常常表现不佳，既可能采取过于保守的态度，也可能在不安全的情况下完成任务，这表明需要更加细致的调校策略。理想情况下，模型应该能够安全地完成任务，即在执行良性请求的部分同时明确指出任何潜在的危害。", "innovation": "本文提出了DUAL-Bench，这是第一个专注于视觉语言模型（VLMs）过度拒绝和安全完成任务的多模态基准测试。该基准针对12类危害进行了评估，并关注模型在保持语义不变的视觉扰动下的稳健性。研究结果显示，现有模型在这方面的表现存在显著提升空间：GPT-5-Nano的安全完成率为12.9%，GPT-5模型平均为7.9%，而Qwen模型仅为3.9%。希望通过DUAL-Bench的研究，促进开发更细致严谨的调校策略，以确保模型在复杂多模态环境中既安全又实用。", "conclusion": "DUAL-Bench的推出旨在推动视觉语言模型的开发工作，促进更细致的调校策略，确保这些模型在未来能够更好地在多模态环境中既安全又有用。"}
{"llm_update_time": "20260203", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2510.08613", "html_url": "https://arxiv.org/abs/2510.08613", "title": "GraphGhost：追踪大型语言模型背后的结构", "title_en": "GraphGhost: Tracing Structures Behind Large Language Models", "authors": "Xinnan Dai,Xianxuan Long,Chung-Hsiang Lo,Kai Guo,Shenglai Zeng,Dongsheng Luo,Jiliang Tang", "background": "大型语言模型（LLMs）在结构化任务上表现出强大的推理能力，但其内部机制仍然知之甚少。现有的解释方法主要集中在令牌级别的归因上，这为理解模型内部多步推理提供了有限的洞察。本文提出GraphGhost，这是一种基于图的框架，将LLMs中的令牌交互和神经激活表示为图。通过跨层追踪令牌之间的依赖性，GraphGhost捕获了模型预测背后的全局信息流。", "innovation": "本文提出GraphGhost框架，通过图模型来表示和分析大型语言模型中令牌之间的交互和神经激活。从样本视角和数据集视角两个方面正式化GraphGhost，通过图分析和定量实验展示了结构属性与起重要作用的令牌和神经节点密切相关，并且结构关键节点的扰动会导致可测量的推理行为变化，从而证明GraphGhost捕获的结构模式反映了LLM推理的实质性内部组织。", "conclusion": "实验结果表明，GraphGhost捕获的结构模式反映了LLM推理的实质性内部组织。所有代码在软件部分提供，且只用于研究目的的材料将提供。"}
{"llm_update_time": "20260203", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2510.08525", "html_url": "https://arxiv.org/abs/2510.08525", "title": "哪些头部对于推理重要？由RL引导的KV缓存压缩", "title_en": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression", "authors": "Wenjie Du,Li Jiang,Keda Tao,Xue Liu,Huan Wang", "background": "大型语言模型在生成过程中展现出了复杂的推理行为，这些行为依赖于扩展的链式思考生成，但由于解码过程中的信息丢失变得非常脆弱，这对KV缓存的压缩造成了巨大的挑战。现有的去Token方法通过直接移除中间步骤来破坏推理链，而专门用于检索任务的Head重分配方法无法保留生成推理所需的头部信息。目前没有方法能够识别哪些注意力头部能够真正保持推理一致性和控制生成终止。", "innovation": "该研究提出了RLKV方法，利用强化学习作为探针，通过优化实际生成结果直接发现对推理质量有贡献的头部。这种方法能够自然地引导出有效的压缩策略：将完整的KV缓存分配给推理关键头部，同时对其他头部进行大胆压缩。实验结果显示，只有部分头部对于推理至关重要，这样可以实现20-50%的缓存减少，同时保持接近无损失的性能，并将速度提升至1.21倍。", "conclusion": "研究表明，一小部分头部对于推理过程极其关键，通过RLKV方法可以实现有效的KV缓存压缩，不仅减少缓存使用，还能大幅提升推理效率。"}
{"llm_update_time": "20260203", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2509.04889", "html_url": "https://arxiv.org/abs/2509.04889", "title": "SpiderNets: 视觉模型从厌恶图像预测人类恐惧", "title_en": "SpiderNets: Vision Models Predict Human Fear From Aversive Images", "authors": "Dominik Pegler,David Steyrl,Mengfan Zhang,Alexander Karner,Jozsef Arato,Frank Scharnowski,Filip Melinscak", "background": "恐惧症很常见且具有破坏性，暴露疗法是最有效的治疗方法，该疗法需要患者面对恐惧刺激的视觉图像。为了使计算机化暴露疗法更具可扩展性，需能够自动化预测恐惧程度，从而适配刺激选择和治疗强度。然而，这种预测是否可靠且能在不同个体和不同图像之间泛化仍不清楚。", "innovation": "研究表明，通过迁移学习调整的预训练卷积和变换器视觉模型能够准确预测与蜘蛛相关的图像的群体感知恐惧度，即使在新的人和新的图像上进行评估时，平均绝对误差（MAE）也低于10。视觉解释分析显示预测主要由图像中的蜘蛛特异性区域驱动。学习曲线分析表明，变换器模型数据效率高，在可用数据量（约300张图像）下接近性能饱和。恐惧预测误差对于极低和极高恐惧水平以及特定类型的图像类别有所增加。", "conclusion": "这些结果证明了从图像透明地、数据驱动地估计恐惧，为自适应数字心理健康工具奠定了基础。"}
{"llm_update_time": "20260203", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2509.16648", "html_url": "https://arxiv.org/abs/2509.16648", "title": "FESTA: 功能等效采样在多模态LLMs可信评估中的应用", "title_en": "FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs", "authors": "Debarpan Bhattacharya,Apoorva Kulkarni,Sriram Ganapathy", "background": "评估多模态大型语言模型（MLLMs）的预测准确性以确保用户信任并促进选择性预测是一项挑战。由于多模态输入的不同表现形式，准确评估的难度增加。现有的评估方法难以全面捕捉模型的不确定性和敏感度。", "innovation": "本文提出了功能等效采样（FESTA），一种多模态输入采样技术，通过基于等效和互补的输入采样生成不确定性度量。FESTA以黑盒方式利用输入-输出访问，无需标注数据进行监督学习，能有效扩展输入空间，探测模型的一致性和敏感性。", "conclusion": "FESTA实现了显著的性能提升，对于视觉LLMs的相对改进为33.3%，对于音频LLMs为29.6%，主要通过面积下ROC曲线（AUROC）检测误预测性能提升。该实现已经开源。"}
{"llm_update_time": "20260203", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2510.04347", "html_url": "https://arxiv.org/abs/2510.04347", "title": "揭示后门：基于梯度注意力异常评分的可解释防御方法", "title_en": "Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models", "authors": "Anindya Sundar Das,Kangjie Chen,Monowar Bhuyan", "background": "预训练语言模型在广泛的自然语言处理（NLP）任务中取得了显著的成功，特别是在经过大规模、领域相关数据集微调后。然而，这些模型仍存在后门攻击的脆弱性，攻击者会在训练数据中植入恶意行为模式。这些触发器在常规使用中是潜伏的，但在激活时会导致目标错误分类。", "innovation": "本文研究了具有触发器的预训练编码器语言模型的内部行为，特别关注处理中毒输入时的一致性注意力和梯度归因偏移；提出了一种推理时间的防御方法，通过结合标记级别的注意力和梯度信息构造异常评分。", "conclusion": "在多种后门攻击场景下的文本分类任务中进行了广泛的实验，表明本文方法显著降低了攻击成功率，相较于现有基准方法。此外，提供了具有解释性的评分机制分析，揭示了触发器位置和所提防御的鲁棒性。"}
{"llm_update_time": "20260203", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2510.02295", "html_url": "https://arxiv.org/abs/2510.02295", "title": "VideoNSA：本土稀疏注意机制扩展视频理解", "title_en": "VideoNSA: Native Sparse Attention Scales Video Understanding", "authors": "Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu", "background": "视频在多模态语言模型中的理解受到了上下文长度的限制：模型往往错过了关键的过渡帧，难以在长时间范围内保持连贯性。", "innovation": "作者通过将本土稀疏注意（NSA）适应到视频-语言模型中，提出了VideoNSA方法。该方法通过端到端训练Qwen2.5-VL模型，并在216K视频指令数据集上进行训练。采用硬件感知的混合注意方式，对文本采用密集注意，对视频则采用NSA。与基于压缩的token压缩方法和无需训练的稀疏基线相比，VideoNSA在理解长视频、时间推理和空间基准测试中表现更优。进一步的消融分析揭示了四个关键发现。", "conclusion": "VideoNSA在最大节点数扩展、固定预算下的全局和局部注意分配、任务相关的分支使用模式以及学习到的组合稀疏注意方面表现出优越性。"}
{"llm_update_time": "20260203", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2509.12396", "html_url": "https://arxiv.org/abs/2509.12396", "title": "网络嵌入中的信息损失与差异效果", "title_en": "Information Loss and Disparate Effects in Network Embeddings", "authors": "Gabriel Chuang,Augustin Chaintreau", "background": "大量的研究集中在了网络嵌入的公平性干预措施上，但对这些嵌入的基线行为了解较少。这篇论文研究了在没有公平性干预措施的条件下，基线嵌入在表示层会产生怎样的不均衡影响。", "innovation": "论文分析了随机块模型图（SBM图）上低维嵌入的渐近行为，编码了同质性和组结构。论文明确了导致信息损失的精确条件，并发现在极限条件下，非常不同的图可以产生相同的嵌入，这种非可逆性对较小和稀疏的社区影响更大。因此，简单的下游任务如链预测为此类社区带来更高的错误率，这有助于解释广泛实践中观察到的不平等现象。", "conclusion": "研究表明，嵌入过程中的信息损失直接取决于图的密度和同质性。不同的图在极限条件下可能产生相同的嵌入，这非可逆性特别影响较小和稀疏的社群。因此，简单的下游任务，如链预测，会对这些社群引入更高的错误率，解释实践中观察到的许多不平等现象。"}
{"llm_update_time": "20260203", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2509.25562", "html_url": "https://arxiv.org/abs/2509.25562", "title": "IRIS: Intrinsic Reward Image Synthesis", "title_en": "IRIS: Intrinsic Reward Image Synthesis", "authors": "Yihang Chen,Yuanhao Ban,Yunqi Hong,Cho-Jui Hsieh", "background": "尽管强化学习从人类反馈（RLHF）在语言推理方面取得了成功，但将其应用于自回归文本到图像（T2I）生成时常受到人类偏好数据有限的限制。本文通过探索自回归T2I模型如何仅依靠内部信号学习，以避免依赖外部奖励或标记数据，对如何改进T2I生成进行了讨论。与最近在数学和代码推理中的发现相反，文章表明，最小化模型的自我确定性而非最大化它，实际上可以提升图像生成的质量。", "innovation": "本文提出了IRIS（内在奖励图像合成），这是第一个仅使用内在奖励利用强化学习来改进自回归T2I模型的框架。实验结果表明，IRIS可以实现优于仅通过外部奖励训练模型的性能，甚至能够媲美通过集成外部奖励训练的模型。此外，IRIS还可促进复杂连贯的推理（CoT）以生成高质量的图像。", "conclusion": "利用IRIS框架，自回归T2I模型的性能得到了显著提升。IRIS不仅能够实现更好的图像生成表现，还能够激励模型进行复杂的推理，以生成更加高质量的图像。"}
{"llm_update_time": "20260203", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2509.21906", "html_url": "https://arxiv.org/abs/2509.21906", "title": "离散流模型的误差分析：基于生成器匹配的方法", "title_en": "Error Analysis of Discrete Flow with Generator Matching", "authors": "Zhengyan Wan,Yidong Ouyang,Qiang Yao,Liyan Xie,Fang Fang,Hongyuan Zha,Guang Cheng", "background": "离散流模型提供了一种学习离散状态空间分布的强大框架，并在性能上优于离散扩散模型。然而，它们的收敛性质和误差分析尚未得到充分研究。", "innovation": "作者利用二类连续时间马尔可夫链（CTMCs）路径测度的吉斯兰诺夫型定理，开发了一个统一的框架，系统地研究了离散流模型的理论性质，提供了针对转换率估计误差和提前停止误差的全面的误差分析。此外，基于生成器匹配和均匀化技术，建立了在无界转换率约束下的无渐近误差界，并在有界转换率约束下得出了估计分布收敛速度更快的结论。", "conclusion": "本研究首次提供离散流模型的误差分析。同时，基于模拟结果探讨了不同设置下的模型性能，取得了快速的总体变异收敛率，几乎达到最优样本大小的相关率。"}
{"llm_update_time": "20260203", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2509.21765", "html_url": "https://arxiv.org/abs/2509.21765", "title": "使用行为巩固的车辆路线问题的终身学习", "title_en": "Lifelong Learning with Behavior Consolidation for Vehicle Routing", "authors": "Jiyuan Pei,Yi Mei,Jialin Liu,Mengjie Zhang,Xin Yao", "background": "近期神经网络的求解器在学习解决路由问题方面显示出有前景的表现，但现有研究主要基于单次训练。一个新任务出现时，要么依赖于零样本泛化，由于新任务和训练任务之间存在差异，这可能会导致泛化效果不佳，要么依赖于重新训练前训练的求解器，这可能造成先前学到的知识的灾难性遗忘。", "innovation": "提出了一种新颖的神经网络求解器的终身学习范式，即LLR-BC（终身学习路由器的行为巩固），其能够解决新任务的同时保持对先前任务学习结果的性能。通过决策寻求的方式，有效地整合了前馈知识，给予低置信度决策更大的整合权重，从而聚焦于重要的经验。", "conclusion": "实验结果表明，LLR-BC在终身学习的框架下有效训练高性能的神经求解器，解决了灾难性遗忘问题，保持了其可塑性，增强了零样本泛化能力。"}
{"llm_update_time": "20260203", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2509.21087", "html_url": "https://arxiv.org/abs/2509.21087", "title": "现代语音增强系统是否易受对抗攻击的影响？", "title_en": "Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?", "authors": "Rostislav Makarov,Lea Schönherr,Timo Gerkmann", "background": "机器学习方法在语音增强方面的应用越来越丰富，使得对输入信号进行复杂修改成为可能。本文发现，这种表达能力引入了一个弱点：高级语音增强模型可能会受到对抗性攻击的影响。特别地，研究表明，精心设计并通过原始输入进行心理声学掩蔽的对抗性噪音可以被注入，从而产生完全不同的语义含义的增强语音输出。实验验证了当前的预测性语音增强模型确实可以被以这种方式操纵。此外，还指出具有随机采样器的扩散模型本身在设计上就具备对这种对抗性攻击的内在抵抗力。", "innovation": "研究发现，先进的语音增强模型可能受到对抗性攻击的影响，提出了一种通过精心设计并心理声学掩蔽的对抗性噪音改变增强语音输出语义的技术。实验验证了这一发现，并指出扩散模型在设计上本身就具备抵御此类攻击的能力。", "conclusion": "现代语音增强系统可能存在被对抗性攻击操纵的风险，但具有随机采样器的扩散模型在设计上就能抵抗这类攻击。"}
{"llm_update_time": "20260203", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2509.22122", "html_url": "https://arxiv.org/abs/2509.22122", "title": "直接估计平均处理效应中的偏差校正项", "title_en": "Direct Bias-Correction Term Estimation for Average Treatment Effect Estimation", "authors": "Masahiro Kato", "background": "研究考虑了估计平均处理效应（ATE）时直接偏差校正项的估计问题。基于观测数据 $\\{(X_i, D_i, Y_i)\\}_{i=1}^{n}$，其中 $X_i$ 代表 $K$ 维协变量，$D_i \\\\in \\\\[0, 1]\\\\$ 为二元处理赋值指示符，$Y_i$ 为结果，文献中提到的偏差校正项 $h_0(D_i, X_i)$ 也被称为 Riesz 表示或聪明协变量，在构造高效的 ATE 估计器方面起着重要作用。", "innovation": "该研究提出了一种新的方法，通过直接最小化偏差校正项模型与其真实值 $h_0$ 之间的 Bregman 散度（包括平方误差和 Kullback-Leibler 散度）来进行偏差校正项的估计，这种方法扩展了现有的偏差校正项估计方法，如协变量平衡权重、Riesz 回归和最近邻匹配。重要的是，在特定的选择偏倚校正项模型和 Bregman 散度的情况下，可以自动保证协变量平衡的性质。", "conclusion": "因此，该研究提供了一种实用的建模和估计方法，通过现有方法的推广实现。"}
